{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoltan23/Audio-Classification/blob/master/audio_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHalG5gdeCWC",
        "colab_type": "code",
        "colab": {},
        "outputId": "61598418-b82a-455d-d28c-bb21a6ed1aa0"
      },
      "source": [
        "\n",
        "!pip install python_speech_features\n",
        "!pip install scipy\n",
        "!pip install pandas\n",
        "!pip install keras\n",
        "!pip install sklearn\n",
        "!pip install tqdm\n",
        "!pip install librosa\n",
        "!apt-get install libsndfile1 -y\n",
        "!apt-get install git -y\n",
        "\n",
        "# !rm -R Audio-Classification\n",
        "\n",
        "\n",
        "!mkdir music-wav-files/clean\n",
        "!mkdir music-wav-files/samples\n",
        "!mkdir music-wav-files/models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python_speech_features in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.17.2)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (5.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.36.1)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/lib/python3/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.46.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.10.2)\n",
            "Requirement already satisfied: llvmlite>=0.30.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa) (0.30.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa) (1.13.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.19)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.28-4ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "mkdir: cannot create directory ‘music-wav-files/clean’: File exists\n",
            "mkdir: cannot create directory ‘music-wav-files/samples’: File exists\n",
            "mkdir: cannot create directory ‘music-wav-files/models’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZP8e54UeCWK",
        "colab_type": "code",
        "colab": {},
        "outputId": "7528b8ce-9753-48e7-b3cd-99921514ef50"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from scipy.io import wavfile as wav\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLwC6kzQeCWQ",
        "colab_type": "text"
      },
      "source": [
        "# The following code was extracted from a module that was created to train a CNN model.  It will be called to retrain the model to improve accuracy as more data becomes available from the app.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CjxI98keCWR",
        "colab_type": "code",
        "colab": {},
        "outputId": "225e81f5-c9eb-4732-b3be-d30ca5937a47"
      },
      "source": [
        "def extract_features(file_name):\n",
        "\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
        "\n",
        "        mask = envelope(audio, sample_rate, 0.0005)\n",
        "\n",
        "        mfccs = librosa.feature.mfcc(y=audio[mask], sr=sample_rate, n_mfcc=40)\n",
        "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
        "\n",
        "        #print(\"mfccsscaled\", mfccsscaled.shape)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None \n",
        "    \n",
        "    return mfccsscaled\n",
        "\n",
        "def envelope(y, rate, threshold):\n",
        "    mask = []\n",
        "    y = pd.Series(y).apply(np.abs)\n",
        "    y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()\n",
        "    for mean in y_mean:\n",
        "        if mean > threshold:\n",
        "            mask.append(True)\n",
        "        else:\n",
        "            mask.append(False)\n",
        "    return mask\n",
        "\n",
        "fulldatasetpath = '/tf/desktop/music-wav-files/wavfiles/'\n",
        "\n",
        "metadata = pd.read_csv('/tf/desktop/music-wav-files/instruments.csv')\n",
        "\n",
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features \n",
        "for index, row in metadata.iterrows():\n",
        "\n",
        "    file_name = os.path.join(os.path.abspath(fulldatasetpath), str(row[\"fname\"]))\n",
        "\n",
        "    class_label = row[\"label\"]\n",
        "    data = extract_features(file_name)\n",
        "    features.append([data, class_label])\n",
        "\n",
        "    # Convert into a Panda dataframe \n",
        "    featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
        "\n",
        "    #print('Finished feature extraction from ', len(featuresdf), ' files') \n",
        "\n",
        "\n",
        "# Convert features and corresponding classification labels into numpy arrays\n",
        "X = np.array(featuresdf.feature.tolist())\n",
        "y = np.array(featuresdf.class_label.tolist())\n",
        "\n",
        "# Encode the classification labels\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y))\n",
        "\n",
        "# split the dataset \n",
        "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
        "\n",
        "num_labels = yy.shape[1]\n",
        "print(\"num_labels\", num_labels)\n",
        "filter_size = 2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(40,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
        "\n",
        "# Display model architecture summary \n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
        "\n",
        "num_epochs = 15\n",
        "num_batch_size = 8\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "#callbacks=[checkpointer],\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test),  verbose=1)\n",
        "model.save('/tf/desktop/music-wav-files/models/model4.h5')\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)\n",
        "\n",
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished feature extraction from  1  files\n",
            "Finished feature extraction from  2  files\n",
            "Finished feature extraction from  3  files\n",
            "Finished feature extraction from  4  files\n",
            "Finished feature extraction from  5  files\n",
            "Finished feature extraction from  6  files\n",
            "Finished feature extraction from  7  files\n",
            "Finished feature extraction from  8  files\n",
            "Finished feature extraction from  9  files\n",
            "Finished feature extraction from  10  files\n",
            "Finished feature extraction from  11  files\n",
            "Finished feature extraction from  12  files\n",
            "Finished feature extraction from  13  files\n",
            "Finished feature extraction from  14  files\n",
            "Finished feature extraction from  15  files\n",
            "Finished feature extraction from  16  files\n",
            "Finished feature extraction from  17  files\n",
            "Finished feature extraction from  18  files\n",
            "Finished feature extraction from  19  files\n",
            "Finished feature extraction from  20  files\n",
            "Finished feature extraction from  21  files\n",
            "Finished feature extraction from  22  files\n",
            "Finished feature extraction from  23  files\n",
            "Finished feature extraction from  24  files\n",
            "Finished feature extraction from  25  files\n",
            "Finished feature extraction from  26  files\n",
            "Finished feature extraction from  27  files\n",
            "Finished feature extraction from  28  files\n",
            "Finished feature extraction from  29  files\n",
            "Finished feature extraction from  30  files\n",
            "Finished feature extraction from  31  files\n",
            "Finished feature extraction from  32  files\n",
            "Finished feature extraction from  33  files\n",
            "Finished feature extraction from  34  files\n",
            "Finished feature extraction from  35  files\n",
            "Finished feature extraction from  36  files\n",
            "Finished feature extraction from  37  files\n",
            "Finished feature extraction from  38  files\n",
            "Finished feature extraction from  39  files\n",
            "Finished feature extraction from  40  files\n",
            "Finished feature extraction from  41  files\n",
            "Finished feature extraction from  42  files\n",
            "Finished feature extraction from  43  files\n",
            "Finished feature extraction from  44  files\n",
            "Finished feature extraction from  45  files\n",
            "Finished feature extraction from  46  files\n",
            "Finished feature extraction from  47  files\n",
            "Finished feature extraction from  48  files\n",
            "Finished feature extraction from  49  files\n",
            "Finished feature extraction from  50  files\n",
            "Finished feature extraction from  51  files\n",
            "Finished feature extraction from  52  files\n",
            "Finished feature extraction from  53  files\n",
            "Finished feature extraction from  54  files\n",
            "Finished feature extraction from  55  files\n",
            "Finished feature extraction from  56  files\n",
            "Finished feature extraction from  57  files\n",
            "Finished feature extraction from  58  files\n",
            "Finished feature extraction from  59  files\n",
            "Finished feature extraction from  60  files\n",
            "Finished feature extraction from  61  files\n",
            "Finished feature extraction from  62  files\n",
            "Finished feature extraction from  63  files\n",
            "Finished feature extraction from  64  files\n",
            "Finished feature extraction from  65  files\n",
            "Finished feature extraction from  66  files\n",
            "Finished feature extraction from  67  files\n",
            "Finished feature extraction from  68  files\n",
            "Finished feature extraction from  69  files\n",
            "Finished feature extraction from  70  files\n",
            "Finished feature extraction from  71  files\n",
            "Finished feature extraction from  72  files\n",
            "Finished feature extraction from  73  files\n",
            "Finished feature extraction from  74  files\n",
            "Finished feature extraction from  75  files\n",
            "Finished feature extraction from  76  files\n",
            "Finished feature extraction from  77  files\n",
            "Finished feature extraction from  78  files\n",
            "Finished feature extraction from  79  files\n",
            "Finished feature extraction from  80  files\n",
            "Finished feature extraction from  81  files\n",
            "Finished feature extraction from  82  files\n",
            "Finished feature extraction from  83  files\n",
            "Finished feature extraction from  84  files\n",
            "Finished feature extraction from  85  files\n",
            "Finished feature extraction from  86  files\n",
            "Finished feature extraction from  87  files\n",
            "Finished feature extraction from  88  files\n",
            "Finished feature extraction from  89  files\n",
            "Finished feature extraction from  90  files\n",
            "Finished feature extraction from  91  files\n",
            "Finished feature extraction from  92  files\n",
            "Finished feature extraction from  93  files\n",
            "Finished feature extraction from  94  files\n",
            "Finished feature extraction from  95  files\n",
            "Finished feature extraction from  96  files\n",
            "Finished feature extraction from  97  files\n",
            "Finished feature extraction from  98  files\n",
            "Finished feature extraction from  99  files\n",
            "Finished feature extraction from  100  files\n",
            "Finished feature extraction from  101  files\n",
            "Finished feature extraction from  102  files\n",
            "Finished feature extraction from  103  files\n",
            "Finished feature extraction from  104  files\n",
            "Finished feature extraction from  105  files\n",
            "Finished feature extraction from  106  files\n",
            "Finished feature extraction from  107  files\n",
            "Finished feature extraction from  108  files\n",
            "Finished feature extraction from  109  files\n",
            "Finished feature extraction from  110  files\n",
            "Finished feature extraction from  111  files\n",
            "Finished feature extraction from  112  files\n",
            "Finished feature extraction from  113  files\n",
            "Finished feature extraction from  114  files\n",
            "Finished feature extraction from  115  files\n",
            "Finished feature extraction from  116  files\n",
            "Finished feature extraction from  117  files\n",
            "Finished feature extraction from  118  files\n",
            "Finished feature extraction from  119  files\n",
            "Finished feature extraction from  120  files\n",
            "Finished feature extraction from  121  files\n",
            "Finished feature extraction from  122  files\n",
            "Finished feature extraction from  123  files\n",
            "Finished feature extraction from  124  files\n",
            "Finished feature extraction from  125  files\n",
            "Finished feature extraction from  126  files\n",
            "Finished feature extraction from  127  files\n",
            "Finished feature extraction from  128  files\n",
            "Finished feature extraction from  129  files\n",
            "Finished feature extraction from  130  files\n",
            "Finished feature extraction from  131  files\n",
            "Finished feature extraction from  132  files\n",
            "Finished feature extraction from  133  files\n",
            "Finished feature extraction from  134  files\n",
            "Finished feature extraction from  135  files\n",
            "Finished feature extraction from  136  files\n",
            "Finished feature extraction from  137  files\n",
            "Finished feature extraction from  138  files\n",
            "Finished feature extraction from  139  files\n",
            "Finished feature extraction from  140  files\n",
            "Finished feature extraction from  141  files\n",
            "Finished feature extraction from  142  files\n",
            "Finished feature extraction from  143  files\n",
            "Finished feature extraction from  144  files\n",
            "Finished feature extraction from  145  files\n",
            "Finished feature extraction from  146  files\n",
            "Finished feature extraction from  147  files\n",
            "Finished feature extraction from  148  files\n",
            "Finished feature extraction from  149  files\n",
            "Finished feature extraction from  150  files\n",
            "Finished feature extraction from  151  files\n",
            "Finished feature extraction from  152  files\n",
            "Finished feature extraction from  153  files\n",
            "Finished feature extraction from  154  files\n",
            "Finished feature extraction from  155  files\n",
            "Finished feature extraction from  156  files\n",
            "Finished feature extraction from  157  files\n",
            "Finished feature extraction from  158  files\n",
            "Finished feature extraction from  159  files\n",
            "Finished feature extraction from  160  files\n",
            "Finished feature extraction from  161  files\n",
            "Finished feature extraction from  162  files\n",
            "Finished feature extraction from  163  files\n",
            "Finished feature extraction from  164  files\n",
            "Finished feature extraction from  165  files\n",
            "Finished feature extraction from  166  files\n",
            "Finished feature extraction from  167  files\n",
            "Finished feature extraction from  168  files\n",
            "Finished feature extraction from  169  files\n",
            "Finished feature extraction from  170  files\n",
            "Finished feature extraction from  171  files\n",
            "Finished feature extraction from  172  files\n",
            "Finished feature extraction from  173  files\n",
            "Finished feature extraction from  174  files\n",
            "Finished feature extraction from  175  files\n",
            "Finished feature extraction from  176  files\n",
            "Finished feature extraction from  177  files\n",
            "Finished feature extraction from  178  files\n",
            "Finished feature extraction from  179  files\n",
            "Finished feature extraction from  180  files\n",
            "Finished feature extraction from  181  files\n",
            "Finished feature extraction from  182  files\n",
            "Finished feature extraction from  183  files\n",
            "Finished feature extraction from  184  files\n",
            "Finished feature extraction from  185  files\n",
            "Finished feature extraction from  186  files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished feature extraction from  187  files\n",
            "Finished feature extraction from  188  files\n",
            "Finished feature extraction from  189  files\n",
            "Finished feature extraction from  190  files\n",
            "Finished feature extraction from  191  files\n",
            "Finished feature extraction from  192  files\n",
            "Finished feature extraction from  193  files\n",
            "Finished feature extraction from  194  files\n",
            "Finished feature extraction from  195  files\n",
            "Finished feature extraction from  196  files\n",
            "Finished feature extraction from  197  files\n",
            "Finished feature extraction from  198  files\n",
            "Finished feature extraction from  199  files\n",
            "Finished feature extraction from  200  files\n",
            "Finished feature extraction from  201  files\n",
            "Finished feature extraction from  202  files\n",
            "Finished feature extraction from  203  files\n",
            "Finished feature extraction from  204  files\n",
            "Finished feature extraction from  205  files\n",
            "Finished feature extraction from  206  files\n",
            "Finished feature extraction from  207  files\n",
            "Finished feature extraction from  208  files\n",
            "Finished feature extraction from  209  files\n",
            "Finished feature extraction from  210  files\n",
            "Finished feature extraction from  211  files\n",
            "Finished feature extraction from  212  files\n",
            "Finished feature extraction from  213  files\n",
            "Finished feature extraction from  214  files\n",
            "Finished feature extraction from  215  files\n",
            "Finished feature extraction from  216  files\n",
            "Finished feature extraction from  217  files\n",
            "Finished feature extraction from  218  files\n",
            "Finished feature extraction from  219  files\n",
            "Finished feature extraction from  220  files\n",
            "Finished feature extraction from  221  files\n",
            "Finished feature extraction from  222  files\n",
            "Finished feature extraction from  223  files\n",
            "Finished feature extraction from  224  files\n",
            "Finished feature extraction from  225  files\n",
            "Finished feature extraction from  226  files\n",
            "Finished feature extraction from  227  files\n",
            "Finished feature extraction from  228  files\n",
            "Finished feature extraction from  229  files\n",
            "Finished feature extraction from  230  files\n",
            "Finished feature extraction from  231  files\n",
            "Finished feature extraction from  232  files\n",
            "Finished feature extraction from  233  files\n",
            "Finished feature extraction from  234  files\n",
            "Finished feature extraction from  235  files\n",
            "Finished feature extraction from  236  files\n",
            "Finished feature extraction from  237  files\n",
            "Finished feature extraction from  238  files\n",
            "Finished feature extraction from  239  files\n",
            "Finished feature extraction from  240  files\n",
            "Finished feature extraction from  241  files\n",
            "Finished feature extraction from  242  files\n",
            "Finished feature extraction from  243  files\n",
            "Finished feature extraction from  244  files\n",
            "Finished feature extraction from  245  files\n",
            "Finished feature extraction from  246  files\n",
            "Finished feature extraction from  247  files\n",
            "Finished feature extraction from  248  files\n",
            "Finished feature extraction from  249  files\n",
            "Finished feature extraction from  250  files\n",
            "Finished feature extraction from  251  files\n",
            "Finished feature extraction from  252  files\n",
            "Finished feature extraction from  253  files\n",
            "Finished feature extraction from  254  files\n",
            "Finished feature extraction from  255  files\n",
            "Finished feature extraction from  256  files\n",
            "Finished feature extraction from  257  files\n",
            "Finished feature extraction from  258  files\n",
            "Finished feature extraction from  259  files\n",
            "Finished feature extraction from  260  files\n",
            "Finished feature extraction from  261  files\n",
            "Finished feature extraction from  262  files\n",
            "Finished feature extraction from  263  files\n",
            "Finished feature extraction from  264  files\n",
            "Finished feature extraction from  265  files\n",
            "Finished feature extraction from  266  files\n",
            "Finished feature extraction from  267  files\n",
            "Finished feature extraction from  268  files\n",
            "Finished feature extraction from  269  files\n",
            "Finished feature extraction from  270  files\n",
            "Finished feature extraction from  271  files\n",
            "Finished feature extraction from  272  files\n",
            "Finished feature extraction from  273  files\n",
            "Finished feature extraction from  274  files\n",
            "Finished feature extraction from  275  files\n",
            "Finished feature extraction from  276  files\n",
            "Finished feature extraction from  277  files\n",
            "Finished feature extraction from  278  files\n",
            "Finished feature extraction from  279  files\n",
            "Finished feature extraction from  280  files\n",
            "Finished feature extraction from  281  files\n",
            "Finished feature extraction from  282  files\n",
            "Finished feature extraction from  283  files\n",
            "Finished feature extraction from  284  files\n",
            "Finished feature extraction from  285  files\n",
            "Finished feature extraction from  286  files\n",
            "Finished feature extraction from  287  files\n",
            "Finished feature extraction from  288  files\n",
            "Finished feature extraction from  289  files\n",
            "Finished feature extraction from  290  files\n",
            "Finished feature extraction from  291  files\n",
            "Finished feature extraction from  292  files\n",
            "Finished feature extraction from  293  files\n",
            "Finished feature extraction from  294  files\n",
            "Finished feature extraction from  295  files\n",
            "Finished feature extraction from  296  files\n",
            "Finished feature extraction from  297  files\n",
            "Finished feature extraction from  298  files\n",
            "Finished feature extraction from  299  files\n",
            "Finished feature extraction from  300  files\n",
            "Finished feature extraction from  301  files\n",
            "Finished feature extraction from  302  files\n",
            "Finished feature extraction from  303  files\n",
            "Finished feature extraction from  304  files\n",
            "Finished feature extraction from  305  files\n",
            "Finished feature extraction from  306  files\n",
            "Finished feature extraction from  307  files\n",
            "Finished feature extraction from  308  files\n",
            "Finished feature extraction from  309  files\n",
            "Finished feature extraction from  310  files\n",
            "Finished feature extraction from  311  files\n",
            "num_labels 11\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               10496     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 11)                2827      \n",
            "=================================================================\n",
            "Total params: 210,699\n",
            "Trainable params: 210,699\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Pre-training accuracy: 11.1111%\n",
            "Train on 248 samples, validate on 63 samples\n",
            "Epoch 1/15\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 8.6587 - accuracy: 0.2944 - val_loss: 4.1704 - val_accuracy: 0.3651\n",
            "Epoch 2/15\n",
            "248/248 [==============================] - 0s 789us/step - loss: 2.3206 - accuracy: 0.4879 - val_loss: 2.0867 - val_accuracy: 0.3651\n",
            "Epoch 3/15\n",
            "248/248 [==============================] - 0s 880us/step - loss: 1.5372 - accuracy: 0.5766 - val_loss: 2.3893 - val_accuracy: 0.5079\n",
            "Epoch 4/15\n",
            "248/248 [==============================] - 0s 808us/step - loss: 0.9777 - accuracy: 0.6734 - val_loss: 1.9875 - val_accuracy: 0.6190\n",
            "Epoch 5/15\n",
            "248/248 [==============================] - 0s 621us/step - loss: 0.8663 - accuracy: 0.7258 - val_loss: 2.2351 - val_accuracy: 0.5397\n",
            "Epoch 6/15\n",
            "248/248 [==============================] - 0s 902us/step - loss: 0.8261 - accuracy: 0.7258 - val_loss: 2.5394 - val_accuracy: 0.4921\n",
            "Epoch 7/15\n",
            "248/248 [==============================] - 0s 868us/step - loss: 0.7371 - accuracy: 0.7863 - val_loss: 1.6237 - val_accuracy: 0.5238\n",
            "Epoch 8/15\n",
            "248/248 [==============================] - 0s 879us/step - loss: 0.3994 - accuracy: 0.8306 - val_loss: 1.8541 - val_accuracy: 0.6032\n",
            "Epoch 9/15\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4829 - accuracy: 0.8468 - val_loss: 2.1041 - val_accuracy: 0.5556\n",
            "Epoch 10/15\n",
            "248/248 [==============================] - 0s 879us/step - loss: 0.4968 - accuracy: 0.8306 - val_loss: 1.9773 - val_accuracy: 0.5397\n",
            "Epoch 11/15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.8710 - val_loss: 2.6032 - val_accuracy: 0.4762\n",
            "Epoch 12/15\n",
            "248/248 [==============================] - 0s 838us/step - loss: 0.4363 - accuracy: 0.8589 - val_loss: 2.0315 - val_accuracy: 0.5397\n",
            "Epoch 13/15\n",
            "248/248 [==============================] - 0s 621us/step - loss: 0.3998 - accuracy: 0.8750 - val_loss: 1.9105 - val_accuracy: 0.6190\n",
            "Epoch 14/15\n",
            "248/248 [==============================] - 0s 922us/step - loss: 0.5772 - accuracy: 0.8468 - val_loss: 2.3752 - val_accuracy: 0.5556\n",
            "Epoch 15/15\n",
            "248/248 [==============================] - 0s 776us/step - loss: 0.5622 - accuracy: 0.7984 - val_loss: 2.2168 - val_accuracy: 0.5397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Can't decrement id ref count (file write failed: time = Mon Oct 28 23:38:47 2019\n, filename = '/tf/desktop/music-wav-files/models/model4.h5', file descriptor = 59, errno = 28, error message = 'No space left on device', buf = 0x7381408, total write size = 40960, bytes this sub-write = 40960, bytes actually written = 18446744073709551615, offset = 9384)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.ObjectID.__dealloc__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't decrement id ref count (file write failed: time = Mon Oct 28 23:38:47 2019\n, filename = '/tf/desktop/music-wav-files/models/model4.h5', file descriptor = 59, errno = 28, error message = 'No space left on device', buf = 0x7381408, total write size = 40960, bytes this sub-write = 40960, bytes actually written = 18446744073709551615, offset = 9384)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqyPXU1UeCWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(filename):\n",
        "    #model = load_model('/tf/desktop/music-wav-files/models/model1.h5')\n",
        "    model = load_model('/tf/desktop/music-wav-files/models/model3.h5')\n",
        "\n",
        "    def extract_feature(file_name):\n",
        "   \n",
        "        try:\n",
        "            audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
        "            mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
        "            mfccsscaled = np.mean(mfccs.T,axis=0)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(\"Error encountered while parsing file: \", file_name)\n",
        "            return None, None\n",
        "        print(\"np.array\", np.array([mfccsscaled]))\n",
        "        return np.array([mfccsscaled])\n",
        "\n",
        "    #Get class labels for predicted_instrument output\n",
        "    le = LabelEncoder()\n",
        "    metadata = pd.read_csv('/tf/desktop/music-wav-files/instruments.csv')\n",
        "    le.fit(metadata['label'])\n",
        "\n",
        "    def print_prediction(file_name):\n",
        "  \n",
        "        prediction_feature = extract_feature(file_name) \n",
        "\n",
        "        predicted_vector = model.predict_classes(prediction_feature)\n",
        "        #print(\"predicted_vector\", predicted_vector)\n",
        "\n",
        "        predicted_class = le.inverse_transform(predicted_vector)\n",
        "        predicted_instrument = predicted_class[0]\n",
        "        print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
        "\n",
        "        predicted_proba_vector = model.predict_proba(prediction_feature) \n",
        "        predicted_proba = predicted_proba_vector[0]\n",
        "        for i in range(len(predicted_proba)):\n",
        "            category = le.inverse_transform(np.array([i]))\n",
        "            print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )\n",
        "            \n",
        "        return predicted_instrument\n",
        "\n",
        "    \n",
        "    return print_prediction(filename)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lQi3a9jeCWc",
        "colab_type": "text"
      },
      "source": [
        "# The following code was extracted from the unit tests we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxMdAf7JeCWd",
        "colab_type": "code",
        "colab": {},
        "outputId": "0312028b-0d02-4920-8dde-e74619e30a9f"
      },
      "source": [
        "def predictedInstrumentTest(filename, instrument):\n",
        "\n",
        "    predicted_instrument = classify(filename)\n",
        "    print(\"The predicted instrument is\", predicted_instrument, \". The actual instrument is\", instrument, \".\")\n",
        "    if (instrument == predicted_instrument):\n",
        "        print(\"Test passed!!!\")\n",
        "    else:\n",
        "        print(\"Test Failed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mIkD6ePeCWi",
        "colab_type": "code",
        "colab": {},
        "outputId": "4981cb5d-e8ea-4110-80ba-c0b323162130"
      },
      "source": [
        "predictedInstrumentTest('/tf/desktop/cl_C4.wav', 'Clarinet')\n",
        "predictedInstrumentTest('/tf/desktop/BbClarinet.wav', 'Clarinet')\n",
        "predictedInstrumentTest('/tf/desktop/trumpet4.wav', 'Trumpet')\n",
        "predictedInstrumentTest('/tf/desktop/trumpet5.wav', 'Trumpet')\n",
        "predictedInstrumentTest('/tf/desktop/g_sharp_real_mono.wav', 'Trumpet')\n",
        "predictedInstrumentTest('/tf/desktop/trumpet_real2.wav', 'Trumpet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e84ab6543f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/cl_C4.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Clarinet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/BbClarinet.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Clarinet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/trumpet4.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Trumpet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/trumpet5.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Trumpet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/g_sharp_real_mono.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Trumpet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a5f845579b5a>\u001b[0m in \u001b[0;36mpredictedInstrumentTest\u001b[0;34m(filename, instrument)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredictedInstrumentTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstrument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredicted_instrument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The predicted instrument is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_instrument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\". The actual instrument is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstrument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minstrument\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredicted_instrument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cd5611840dcd>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#model = load_model('/tf/desktop/music-wav-files/models/model1.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/desktop/music-wav-files/models/model3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u4tkcEVeCWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN8Xc5T9eCWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}